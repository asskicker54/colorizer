{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device_count {\n",
       "  key: \"CPU\"\n",
       "  value: 4\n",
       "}\n",
       "device_count {\n",
       "  key: \"GPU\"\n",
       "  value: 1\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.ConfigProto(device_count = {'GPU': 1 , 'CPU': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 120\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../datasets/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for image_file in os.listdir(data_dir)[0: 2500]:\n",
    "    rgb_image = Image.open(os.path.join(data_dir, image_file)).resize((IMG_SIZE, IMG_SIZE))\n",
    "    # Нормализация цветного изображения\n",
    "    rgb_image_list = np.asarray(rgb_image) / 255\n",
    "    grayscale_image = rgb_image.convert('L')\n",
    "    # Нормализация черно-белого изображения\n",
    "    grayscale_image_list = (np.asarray(grayscale_image).reshape(IMG_SIZE, IMG_SIZE, 1)) / 255\n",
    "\n",
    "    x.append(grayscale_image)\n",
    "    y.append(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(x), np.array(y), test_size=0.1)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генеративно-состязательная сеть (GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_model():\n",
    "\n",
    "    inputs = tf.keras.layers.Input( shape=( IMG_SIZE , IMG_SIZE , 1 ) )\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv2D( 16 , kernel_size=( 5 , 5 ) , strides=1 )( inputs )\n",
    "    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n",
    "    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )\n",
    "    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n",
    "    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )\n",
    "    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D( 32 , kernel_size=( 5 , 5 ) , strides=1)( conv1 )\n",
    "    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n",
    "    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 )( conv2 )\n",
    "    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n",
    "    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 )( conv2 )\n",
    "    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1 )( conv2 )\n",
    "    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n",
    "    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( conv3 )\n",
    "    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n",
    "    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( conv3 )\n",
    "    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n",
    "\n",
    "    bottleneck = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='tanh' , padding='same' )( conv3 )\n",
    "\n",
    "    concat_1 = tf.keras.layers.Concatenate()( [ bottleneck , conv3 ] )\n",
    "    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_1 )\n",
    "    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( conv_up_3 )\n",
    "    conv_up_3 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_3 )\n",
    "\n",
    "    concat_2 = tf.keras.layers.Concatenate()( [ conv_up_3 , conv2 ] )\n",
    "    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_2 )\n",
    "    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( conv_up_2 )\n",
    "    conv_up_2 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_2 )\n",
    "\n",
    "    concat_3 = tf.keras.layers.Concatenate()( [ conv_up_2 , conv1 ] )\n",
    "    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( concat_3 )\n",
    "    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( conv_up_1 )\n",
    "    conv_up_1 = tf.keras.layers.Conv2DTranspose( 3 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu')( conv_up_1 )\n",
    "\n",
    "    model = tf.keras.models.Model( inputs , conv_up_1 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_model():\n",
    "    layers = [\n",
    "        tf.keras.layers.Conv2D( 32 , kernel_size=( 7 , 7 ) , strides=1 , activation='relu' , input_shape=( 120 , 120 , 3 ) ),\n",
    "        tf.keras.layers.Conv2D( 32 , kernel_size=( 7, 7 ) , strides=1, activation='relu'  ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1, activation='relu'  ),\n",
    "        tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1, activation='relu'  ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),\n",
    "        tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),\n",
    "        tf.keras.layers.Conv2D( 256 , kernel_size=( 3 , 3 ) , strides=1, activation='relu'  ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense( 512, activation='relu'  )  ,\n",
    "        tf.keras.layers.Dense( 128 , activation='relu' ) ,\n",
    "        tf.keras.layers.Dense( 16 , activation='relu' ) ,\n",
    "        tf.keras.layers.Dense( 1 , activation='sigmoid' ) \n",
    "    ]\n",
    "    model = tf.keras.models.Sequential( layers )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output) - tf.random.uniform( shape=real_output.shape , maxval=0.1 ) , real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output) + tf.random.uniform( shape=fake_output.shape , maxval=0.1  ) , fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output , real_y):\n",
    "    real_y = tf.cast( real_y , 'float32' )\n",
    "    return mse( fake_output , real_y )\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam( 0.0005 )\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam( 0.0005 )\n",
    "\n",
    "generator = get_generator_model()\n",
    "discriminator = get_discriminator_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step( input_x , real_y ):\n",
    "   \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generate an image -> G( x )\n",
    "        with tf.device('GPU'):\n",
    "            generated_images = generator( input_x , training=True)\n",
    "        # Probability that the given image is real -> D( x )\n",
    "            real_output = discriminator( real_y, training=True)\n",
    "        # Probability that the given image is the one generated -> D( G( x ) )\n",
    "            generated_output = discriminator(generated_images, training=True)\n",
    "        \n",
    "        # L2 Loss -> || y - G(x) ||^2\n",
    "        with tf.device('CPU'):\n",
    "            gen_loss = generator_loss( generated_images , real_y )\n",
    "        # Log loss for the discriminator\n",
    "            disc_loss = discriminator_loss( real_output, generated_output )\n",
    "    \n",
    "    #tf.keras.backend.print_tensor( tf.keras.backend.mean( gen_loss ) )\n",
    "    #tf.keras.backend.print_tensor( gen_loss + disc_loss )\n",
    "\n",
    "    # Compute the gradients\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Optimize with Adam\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save_weights('gen_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
